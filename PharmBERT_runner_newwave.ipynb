{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27549,"status":"ok","timestamp":1730114569072,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"mp6SuHws7DDv","outputId":"681995d4-e932-4849-df88-2fcbeac43a01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Collecting datasets\n","  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n","/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install datasets\n","\n","%cd /content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1730114569574,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"QqdkEeys7jKS","outputId":"1652ca6a-c576-4112-855a-e4fbc2cbb3a0"},"outputs":[{"name":"stdout","output_type":"stream","text":[" all_entries_with_labels.pkl\t\t logs\n"," augmenter.ipynb\t\t\t old_PharmBERT_runner_final.ipynb\n"," biomedical_ner_all_runner_final.ipynb\t output\n"," concatenated.json\t\t\t PharmBERT_runner_newwave.ipynb\n"," concatenated.txt\t\t\t pickle_opener.ipynb\n"," data_maker.ipynb\t\t\t reason_reader.ipynb\n"," example\t\t\t\t results\n"," fine_tuned_biobert\t\t\t runner.ipynb\n"," finetuned_biodemoical_ner_model\t run_tester.ipynb\n"," finetuned_pharmbert_model\t\t split_finetuned_pharmbert_model\n"," ground_truth_labeller.ipynb\t\t test\n"," input\t\t\t\t\t text_concaternator.ipynb\n"," labeled_records\t\t\t tokenized_data.pkl\n","'labeller copy.ipynb'\t\t\t tokenized_input_txt_maker.ipynb\n"," labeller.ipynb\t\t\t\t tokenizer.ipynb\n"," langchain.ipynb\t\t\t unique_values.json\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":14634,"status":"ok","timestamp":1730114584206,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"GU0GJG-arsfD"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/remon.m/miniconda3/envs/PharmBERT/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# General imports\n","import os\n","import re\n","import json\n","import random\n","import string\n","import numpy as np\n","from pathlib import Path\n","from typing import List\n","\n","# PyTorch imports\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.nn import CrossEntropyLoss\n","\n","# Hugging Face Transformers\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForTokenClassification,\n","    PreTrainedTokenizerBase\n",")\n","\n","# Hugging Face Datasets\n","from datasets import Dataset\n","\n","# Scikit-learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from sklearn.metrics import classification_report\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730114584206,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"NUlv0dQV8b1L"},"outputs":[],"source":["# # Define input and output directories - COLAB\n","input_dir = '/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth/input' #change for test/ actual\n","output_dir = '/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth/output' #change for test/ actual\n","unique_values_json = '/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth/unique_values.json'"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7QHlUlwS8dK2"},"outputs":[],"source":["# Define input and output directories - LOCAL\n","# input_dir = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/input' #change for test/ actual\n","# output_dir = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/output' #change for test/ actual\n","# unique_values_json = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/unique_values.json'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":897,"status":"ok","timestamp":1730114585097,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"9x_HPMePf2ld"},"outputs":[],"source":["# Load unique values from JSON\n","with open(unique_values_json, 'r', encoding='utf-8') as json_file:\n","    unique_values = json.load(json_file)\n","\n","reasons_list = [reason.lower().strip() for reason in unique_values['reasons']]\n","drugs_list = [drug.lower().strip() for drug in unique_values['drug_names']]\n","forms_list = [form.lower().strip() for form in unique_values['forms']]\n","units_list = [unit.lower().strip() for unit in unique_values['units']]\n","dosages_list = [dosage.lower().strip() for dosage in unique_values['dosages']]\n","frequencies_list = [freq.lower().strip() for freq in unique_values['frequencies']]\n","routes_list = [route.lower().strip() for route in unique_values['routes']]\n","\n","# Define patterns for different fields\n","reason_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join(re.escape(reason) for reason in reasons_list) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","drug_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join(re.escape(drug) for drug in drugs_list) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","unit_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join([\"mg\", \"mcg\", \"g\", \"ml\", \"%\", \"unit\", \"drop\", \"dose\"] +\n","    [re.escape(u) for u in units_list]) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","form_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join(re.escape(form) for form in forms_list) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","dosage_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join([\"\\d+\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"] +\n","    [re.escape(d) for d in dosages_list]) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","frequency_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join(re.escape(freq) for freq in frequencies_list) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","route_pattern = re.compile(\n","    r\"\\b(\" + \"|\".join(re.escape(route) for route in routes_list) + r\")\\b\",\n","    re.IGNORECASE\n",")\n","\n","# Define label list\n","label_list = [\"O\", \"B-DRUG\", \"I-DRUG\", \"B-STRENGTH\", \"I-STRENGTH\",\n","              \"B-FORM\", \"I-FORM\", \"B-DOSAGE\", \"I-DOSAGE\",\n","              \"B-FREQUENCY\", \"I-FREQUENCY\", \"B-ROUTE\", \"I-ROUTE\",\n","              \"B-REASON\", \"I-REASON\"]\n","\n","# Define the valid fields used for structured data extraction\n","valid_fields = [\"DRUG\", \"STRENGTH\", \"FORM\", \"DOSAGE\", \"FREQUENCY\", \"ROUTE\", \"REASON\"]\n","\n","label_map = {\n","    \"O\": 0,\n","    \"B-DRUG\": 1,\n","    \"I-DRUG\": 2,\n","    \"B-STRENGTH\": 3,\n","    \"I-STRENGTH\": 4,\n","    \"B-FORM\": 5,\n","    \"I-FORM\": 6,\n","    \"B-DOSAGE\": 7,\n","    \"I-DOSAGE\": 8,\n","    \"B-FREQUENCY\": 9,\n","    \"I-FREQUENCY\": 10,\n","    \"B-ROUTE\": 11,\n","    \"I-ROUTE\": 12,\n","    \"B-REASON\": 13,\n","    \"I-REASON\": 14,\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":65888,"status":"ok","timestamp":1730114652050,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"EjD40FZLuJXu"},"outputs":[],"source":["def load_raw_instructions(input_dir):\n","    \"\"\"Load raw instructions from input files.\"\"\"\n","    input_data = {}\n","    for file_name in os.listdir(input_dir):\n","        if file_name.endswith('.txt'):\n","            file_path = os.path.join(input_dir, file_name)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                base_name = file_name.replace('.txt', '')\n","                input_data[base_name] = f.read().strip()\n","    return input_data\n","\n","def load_structured_data(output_dir):\n","    \"\"\"Load and format structured output data.\"\"\"\n","    output_data = {}\n","    for file_name in os.listdir(output_dir):\n","        if file_name.endswith('.txt'):\n","            file_path = os.path.join(output_dir, file_name)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                content = f.read().split('Labels:')[0].strip()\n","                if content.startswith('Extracted Fields:'):\n","                    content = content[len('Extracted Fields:'):].strip()\n","                formatted_content = re.sub(r'(\\d+\\.)\\s+', r'\\n\\1 ', content)\n","                base_name = file_name.replace('_redone_response.txt', '')\n","                output_data[base_name] = formatted_content.strip()\n","    return output_data\n","\n","raw_instructions = load_raw_instructions(input_dir)\n","structured_data = load_structured_data(output_dir)"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730114652051,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"fLOI-KSdsapc"},"outputs":[],"source":["def align_data(input_data, output_data):\n","    \"\"\"Align input and structured output data into a dataset.\"\"\"\n","    dataset = []\n","    not_in = []\n","\n","    for base_name, input_text in input_data.items():\n","        structured_text = output_data.get(base_name)\n","\n","        if not structured_text:\n","            print(f\"Warning: No matching output data found for {base_name}. Skipping.\")\n","            not_in.append(base_name)\n","            continue\n","\n","        # Split input and structured lines properly\n","        input_lines = [line.strip() for line in input_text.split('\\n') if line.strip()]\n","        structured_lines = [line.strip() for line in structured_text.split('\\n')\n","                            if line.strip() and \"Extract\" not in line and \"```\" not in line and \"--\" not in line]\n","\n","        # Check if input lines match the number of structured lines\n","        if len(input_lines) != len(structured_lines):\n","            print(f\"Warning: Mismatch in number of lines for {base_name}. Skipping.\")\n","            print(len(input_lines), len(structured_lines))\n","            not_in.append(base_name)\n","            continue\n","\n","        # Create entries for each input-output pair\n","        for input_line, structured_line in zip(input_lines, structured_lines):\n","            structured_dict = {}\n","\n","            # Only process structured lines that match the expected format\n","            if re.match(r'^\\d+\\.\\s*', structured_line) and \" | \" in structured_line:\n","                fields = [field.split(\": \")[1] if \": \" in field else \"N/A\"\n","                          for field in structured_line.split(\" | \")]\n","\n","                # Ensure fields contains enough elements before accessing them\n","                if len(fields) < len(valid_fields):\n","                    print(input_line, structured_line)\n","                    print(f\"Warning: Mismatch in the number of fields for {base_name}. Skipping.\")\n","                    print(f\"Expected {len(valid_fields)} fields but got {len(fields)}.\")\n","                    not_in.append(base_name)\n","                    continue\n","\n","                drug_name = fields[0]\n","\n","                if drug_name.lower() != \"n/a\":\n","                    structured_dict[drug_name] = {\n","                        valid_field: fields[idx] if idx < len(fields) else \"N/A\"\n","                        for idx, valid_field in enumerate(valid_fields)\n","                    }\n","\n","                # Append the new sample to the dataset\n","                if structured_dict and input_line.lower() != \"expired\":\n","                    dataset.append({\"instructions\": input_line, \"medications\": structured_dict})\n","\n","    return dataset, not_in\n","\n","aligned_data, not_in = align_data(raw_instructions, structured_data)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":448,"status":"ok","timestamp":1730114661809,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"AGtswXY0ii44"},"outputs":[],"source":["# Initialize a set to collect unique words from the data\n","new_tokens_set = set()\n","\n","# Simple space-based tokenizer function for vocabulary extraction\n","def simple_tokenizer(text: str):\n","    return text.split()\n","\n","# Iterate through each entry in the aligned_data\n","for entry in aligned_data:\n","    # Extract the instructions and split into words\n","    instruction_tokens = simple_tokenizer(entry['instructions'])\n","    new_tokens_set.update(instruction_tokens)  # Add tokens to the set\n","\n","    # Extract medication details\n","    medications = entry['medications']\n","\n","    # Iterate through each medication in the entry\n","    for medication_name, fields in medications.items():\n","        # Add the medication name itself\n","        new_tokens_set.add(medication_name)\n","\n","        # Iterate through each field in the medication dictionary\n","        for field_name, field_value in fields.items():\n","            # Split the field value into tokens and add them\n","            field_tokens = simple_tokenizer(field_value)\n","            new_tokens_set.update(field_tokens)\n","\n","# Convert the set to a sorted list to maintain a consistent order\n","new_tokens = sorted(list(new_tokens_set))\n","\n","class SimpleTokenizer:\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","        self.token_to_id = {token: idx + 1 for idx, token in enumerate(vocab)}  # ID starts at 1\n","        self.unknown_token_id = len(self.token_to_id) + 1  # ID for unknown tokens\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        # Use the same splitting approach as simple_tokenizer\n","        return text.split()\n","\n","    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n","        # Handle unknown tokens consistently\n","        return [self.token_to_id.get(token, self.unknown_token_id) for token in tokens]\n","\n","    def __call__(self, text: str):\n","        tokens = self.tokenize(text)\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": [1] * len(input_ids)  # Default attention mask for each token\n","        }\n","\n","    def save_pretrained(self, save_directory):\n","        \"\"\"Save the tokenizer vocabulary to a directory.\"\"\"\n","        if not os.path.exists(save_directory):\n","            os.makedirs(save_directory)\n","\n","        vocab_file = os.path.join(save_directory, 'vocab.json')\n","        with open(vocab_file, 'w') as f:\n","            json.dump(self.token_to_id, f)\n","\n","        print(f\"Tokenizer vocabulary saved to {vocab_file}\")\n","\n","# Initialize the tokenizer with the new vocabulary\n","tokenizer = SimpleTokenizer(vocab=new_tokens)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":28352,"status":"ok","timestamp":1730114692766,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"ZSD6giyij_OB"},"outputs":[],"source":["def get_label_for_word(word, medications, previous_word=None, next_word=None, current_reason=None, reason_tracker=None):\n","    \"\"\"Retrieve the correct label for a word based on context and structured data.\"\"\"\n","    normalized_word = word.lower()\n","\n","    if re.match(r\"^\\d+\\.$\", normalized_word.strip()) or normalized_word.strip() == '.':\n","        return \"O\", reason_tracker  # Not a dosage\n","\n","    # Track multi-word reasons\n","    if reason_tracker:\n","        expected_word = reason_tracker[\"expected_word\"]\n","        full_reason = reason_tracker[\"full_reason\"]\n","        current_word_index = reason_tracker[\"index\"]\n","\n","        if normalized_word.strip(string.punctuation) == expected_word:  # Matches the next word in the multi-word reason\n","            # If it's the last word in the multi-word reason\n","            if current_word_index == len(full_reason) - 1:\n","                reason_tracker = None  # Reset the tracker after last word\n","            else:\n","                reason_tracker[\"index\"] += 1  # Move to the next word in the multi-word reason\n","                reason_tracker[\"expected_word\"] = full_reason[current_word_index]  # Update next word\n","            return \"I-REASON\", reason_tracker  # Continue tagging as I-REASON\n","\n","    # Iterate through reasons to find multi-word reasons\n","    for reason in reasons_list:\n","        reason_words = reason.split()  # Split multi-word reason into individual words\n","        if normalized_word == reason_words[0]:  # First word of the reason\n","            if len(reason_words) > 1:  # It's a multi-word reason\n","                reason_tracker = {\n","                    \"index\": 1,  # Index of the next word to expect\n","                    \"full_reason\": reason_words,  # The full reason broken into words\n","                    \"expected_word\": reason_words[1],  # The next word to look for\n","                }\n","            return \"B-REASON\", reason_tracker  # Start of a multi-word reason\n","\n","    # Existing checks for single-word reasons\n","    if re.match(reason_pattern, normalized_word):\n","        return (\"B-REASON\", reason_tracker) if current_reason != normalized_word else (\"I-REASON\", reason_tracker)\n","\n","    # Handle strength detection (number followed by a unit)\n","    if re.match(dosage_pattern, normalized_word) and next_word and re.match(unit_pattern, next_word.lower()):\n","        return \"B-STRENGTH\", reason_tracker\n","\n","    # Handle I-STRENGTH (continuation with a unit)\n","    if re.match(unit_pattern, normalized_word):\n","        return \"I-STRENGTH\", reason_tracker\n","\n","    # Handle B-DOSAGE (number followed by a form)\n","    if re.match(dosage_pattern, normalized_word) and next_word and re.match(form_pattern, next_word.lower()):\n","        return \"B-DOSAGE\", reason_tracker\n","\n","    # Handle dosage detection based on the previous word (number + unit)\n","    if previous_word and re.match(dosage_pattern, previous_word) and re.match(unit_pattern, normalized_word):\n","        return \"I-DOSAGE\", reason_tracker\n","\n","    # Check if the current word is a dosage value (standalone number)\n","    if re.match(dosage_pattern, normalized_word):\n","        return (\"B-DOSAGE\", reason_tracker) if previous_word not in [\".\", \",\"] else (\"O\", reason_tracker)  # Avoid labeling numbered lists\n","\n","    # Iterate through medications to find a match\n","    for drug, attributes in medications.items():\n","        if drug.lower() in normalized_word:\n","            return \"B-DRUG\", reason_tracker\n","\n","        # Check for other attributes, including reason\n","        for field, value in attributes.items():\n","            if field.upper() == \"REASON\" and value.lower() in normalized_word:\n","                return (\"B-REASON\", reason_tracker) if current_reason != value.lower() else (\"I-REASON\", reason_tracker)\n","            if value.lower() in normalized_word:\n","                return f\"B-{field.upper()}\", reason_tracker\n","\n","    return \"O\", reason_tracker  # Default to 'O' if no match is found\n","\n","def tokenize_instructions(raw_instructions):\n","    \"\"\"Tokenize raw instructions using a simple space-based tokenizer.\"\"\"\n","    tokenized_data = []\n","\n","    for base_name, instruction in raw_instructions.items():\n","        # Tokenize the raw instruction\n","        tokens = tokenizer(instruction)\n","\n","        # Prepare entry for saving\n","        entry = {\n","            \"raw_instruction\": instruction,\n","            \"tokenized_instruction\": tokens['input_ids']  # Use input_ids instead of the whole output\n","        }\n","\n","        tokenized_data.append(entry)\n","\n","    return tokenized_data\n","\n","def tokenize_and_align_for_dict(entry):\n","    \"\"\"Tokenize input text and align labels with tokens using a simple tokenizer.\"\"\"\n","    instructions = entry['instructions']\n","    medications = entry['medications']\n","\n","    # Tokenize the input text using the simple space-based tokenizer\n","    tokens = tokenizer(instructions)  # This returns a dictionary with \"input_ids\"\n","\n","    # Get the token strings from the input_ids\n","    token_strings = [tokenizer.vocab[token_id - 1] for token_id in tokens['input_ids']]  # Convert to strings\n","\n","    aligned_labels = []\n","    current_reason = None  # Track the current reason for multi-word labeling\n","    reason_tracker = None  # Track the multi-word reason state\n","\n","    for idx, token in enumerate(token_strings):\n","        previous_word = token_strings[idx - 1] if idx > 0 else None\n","        next_word = token_strings[idx + 1] if idx < len(token_strings) - 1 else None\n","\n","        label, reason_tracker = get_label_for_word(\n","            token, medications,\n","            previous_word=previous_word,\n","            next_word=next_word,\n","            current_reason=current_reason,\n","            reason_tracker=reason_tracker\n","        )\n","\n","        aligned_labels.append(label_map[label])  # Ensure label_map is defined\n","\n","    return {\"tokens\": token_strings, \"labels\": aligned_labels}\n","\n","# Example usage on the first entry of your aligned data\n","aligned_tokenized_data = [tokenize_and_align_for_dict(entry) for entry in aligned_data]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1730114695590,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"RFQkn5iC636l","outputId":"67f34588-2173-4d9a-a0de-b0c03cb47e3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Entry 1 ---\n","\n","Original Input Text:\n","1. Duloxetine 60 mg Capsule Sig: One (1) Capsule PO DAILY for anxiety.\n","\n","Tokenized Tokens and Corresponding Labels:\n","\n","1. : O\n","Duloxetine : B-DRUG\n","60 : B-STRENGTH\n","mg : I-STRENGTH\n","Capsule : B-FORM\n","Sig: : O\n","One : B-DOSAGE\n","(1) : B-DOSAGE\n","Capsule : B-FORM\n","PO : B-ROUTE\n","DAILY : B-FREQUENCY\n","for : O\n","anxiety. : B-REASON\n","\n","--- Entry 2 ---\n","\n","Original Input Text:\n","2. Levothyroxine 75 mcg Tablet Sig: One (1) Tablet PO DAILY for hypothyroidism.\n","\n","Tokenized Tokens and Corresponding Labels:\n","\n","2. : O\n","Levothyroxine : B-DRUG\n","75 : B-STRENGTH\n","mcg : I-STRENGTH\n","Tablet : B-FORM\n","Sig: : O\n","One : B-DOSAGE\n","(1) : B-DOSAGE\n","Tablet : B-FORM\n","PO : B-ROUTE\n","DAILY : B-FREQUENCY\n","for : O\n","hypothyroidism. : B-REASON\n","\n","--- Entry 3 ---\n","\n","Original Input Text:\n","3. Loratadine 10 mg Tablet Sig: One (1) Tablet PO DAILY for allergies.\n","\n","Tokenized Tokens and Corresponding Labels:\n","\n","3. : O\n","Loratadine : B-DRUG\n","10 : B-STRENGTH\n","mg : I-STRENGTH\n","Tablet : B-FORM\n","Sig: : O\n","One : B-DOSAGE\n","(1) : B-DOSAGE\n","Tablet : B-FORM\n","PO : B-ROUTE\n","DAILY : B-FREQUENCY\n","for : O\n","allergies. : B-REASON\n","\n","Finished displaying the first 3 entries.\n"]}],"source":["# Iterate through the first 3 entries in aligned_tokenized_data\n","for i in range(3):  # random 3 entries from the middle 27,30\n","    print(f\"\\n--- Entry {i + 1} ---\")\n","\n","    # Get the entry and extract tokens and labels\n","    entry = aligned_tokenized_data[i]\n","    tokens = entry[\"tokens\"]  # Access tokens directly\n","    labels = entry[\"labels\"]\n","\n","    # Print the original input text for reference\n","    print(\"\\nOriginal Input Text:\")\n","    print(aligned_data[i]['instructions'])\n","\n","    print(\"\\nTokenized Tokens and Corresponding Labels:\\n\")\n","\n","    # Display tokens with their aligned labels\n","    for token, label_id in zip(tokens, labels):\n","        label = label_list[label_id] if label_id != -100 else \"PAD/SPECIAL\"\n","        print(f\"{token} : {label}\")\n","\n","print(\"\\nFinished displaying the first 3 entries.\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1730114701763,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"_oCWIM5fp0kU"},"outputs":[],"source":["# Example of converting the aligned_data using the SimpleTokenizer\n","updated_data = []\n","\n","for idx, entry in enumerate(aligned_tokenized_data):\n","    if \"tokens\" not in entry:\n","        print(f\"Warning: 'tokens' key missing in entry {idx}. Entry content: {entry}\")\n","        continue\n","\n","    # Tokenize the text and convert tokens to input IDs using the SimpleTokenizer\n","    tokenized_output = tokenizer(' '.join(entry[\"tokens\"]))  # Join tokens with a space and tokenize\n","    input_ids = tokenized_output[\"input_ids\"]\n","    attention_mask = tokenized_output[\"attention_mask\"]\n","\n","    # Create the updated entry with input_ids, attention_mask, and labels\n","    updated_entry = {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\": entry[\"labels\"]\n","    }\n","    updated_data.append(updated_entry)\n","\n","import pickle\n","\n","with open('final_data.pkl', 'wb') as file:\n","    pickle.dump(updated_data, file)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["updated_data\n","\n","with open('aligned_tokenized_data.pkl', 'wb') as file:\n","    pickle.dump(aligned_tokenized_data, file)\n","\n","with open('raw_data.pkl', 'wb') as file:\n","    pickle.dump(aligned_data, file)\n"]},{"cell_type":"code","execution_count":191,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1730114359016,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"NkA7tWd1dJdJ","outputId":"57f77092-2c36-4ea5-9fef-d169fdbfedd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Entry 1:\n","Original Tokens: ['1.', 'Duloxetine', '60', 'mg', 'Capsule', 'Sig:', 'One', '(1)', 'Capsule', 'PO', 'DAILY', 'for', 'depression.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13]\n","\n","Updated Entry 1:\n","Input IDs: [59, 470, 153, 1653, 339, 1068, 897, 1, 339, 912, 413, 1511, 1415]\n","Tokens: ['1.', 'Duloxetine', '60', 'mg', 'Capsule', 'Sig:', 'One', '(1)', 'Capsule', 'PO', 'DAILY', 'for', 'depression.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","--------------------------------------------------\n","Original Entry 2:\n","Original Tokens: ['2.', 'Triptorelin', '3.75', 'mg', 'Injection', 'Sig:', 'One', '(1)', 'Injection', 'Intramuscular', 'Monthly', 'for', 'prostate', 'cancer.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13, 14]\n","\n","Updated Entry 2:\n","Input IDs: [98, 1194, 119, 1653, 674, 1068, 897, 1, 674, 686, 823, 1511, 1800, 1353]\n","Tokens: ['2.', 'Triptorelin', '3.75', 'mg', 'Injection', 'Sig:', 'One', '(1)', 'Injection', 'Intramuscular', 'Monthly', 'for', 'prostate', 'cancer.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13, 14]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","--------------------------------------------------\n","Original Entry 3:\n","Original Tokens: ['3.', 'Levothyroxine', '50', 'mcg', 'Tablet', 'Sig:', 'One', '(1)', 'Tablet', 'PO', 'DAILY', 'for', 'hypothyroidism.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13]\n","\n","Updated Entry 3:\n","Input IDs: [117, 740, 144, 1635, 1130, 1068, 897, 1, 1130, 912, 413, 1511, 1569]\n","Tokens: ['3.', 'Levothyroxine', '50', 'mcg', 'Tablet', 'Sig:', 'One', '(1)', 'Tablet', 'PO', 'DAILY', 'for', 'hypothyroidism.']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","--------------------------------------------------\n"]}],"source":["# Check a sample entry to ensure correct conversion\n","for idx, (original_entry, updated_entry) in enumerate(zip(aligned_tokenized_data[:3], updated_data[:3])):\n","    print(f\"Original Entry {idx + 1}:\")\n","    print(f\"Original Tokens: {original_entry['tokens']}\")\n","    print(f\"Labels: {original_entry['labels']}\\n\")\n","\n","    print(f\"Updated Entry {idx + 1}:\")\n","    print(f\"Input IDs: {updated_entry['input_ids']}\")\n","    print(f\"Tokens: {[new_tokens[i - 1] if i <= len(new_tokens) else '[UNK]' for i in updated_entry['input_ids']]}\")\n","    print(f\"Labels: {updated_entry['labels']}\")\n","    print(f\"Attention Mask: {updated_entry['attention_mask']}\\n\")\n","    print(\"-\" * 50)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1730114707001,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"ZSp--_6DvfLH","outputId":"6055741d-a22d-42a3-d737-2585bb8f7fe7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Tokens: ['1.', 'Duloxetine', '60', 'mg', 'Capsule', 'Sig:', 'One', '(1)', 'Capsule', 'PO', 'DAILY', 'for', 'depression.']\n","Tokenized Output IDs: [59, 470, 153, 1653, 339, 1068, 897, 1, 339, 912, 413, 1511, 1415]\n","Tokenized Output Tokens: ['1.', 'Duloxetine', '60', 'mg', 'Capsule', 'Sig:', 'One', '(1)', 'Capsule', 'PO', 'DAILY', 'for', 'depression.']\n"]}],"source":["# Test tokenizer on sample tokens\n","sample_text = ' '.join(aligned_tokenized_data[0][\"tokens\"])\n","tokenized_output = tokenizer(sample_text)\n","\n","print(f\"Original Tokens: {aligned_tokenized_data[0]['tokens']}\")\n","print(f\"Tokenized Output IDs: {tokenized_output['input_ids']}\")\n","print(f\"Tokenized Output Tokens: {[tokenizer.vocab[idx - 1] if idx - 1 < len(tokenizer.vocab) else '[UNK]' for idx in tokenized_output['input_ids']]}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":355,"status":"ok","timestamp":1730114709478,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"UXodFDQvmKur"},"outputs":[],"source":["# Custom padding function\n","def custom_pad_and_truncate(batch, max_length=48):\n","    \"\"\"Pad and truncate the input IDs, attention masks, and labels.\"\"\"\n","    padded_batch = {\n","        \"input_ids\": [],\n","        \"attention_mask\": [],\n","        \"labels\": []\n","    }\n","\n","    for item in batch:\n","        # Pad or truncate input_ids\n","        input_ids = item['input_ids']\n","        input_ids = input_ids[:max_length] + [0] * max(0, max_length - len(input_ids))  # Pad with 0\n","        padded_batch[\"input_ids\"].append(input_ids)\n","\n","        # Pad or truncate attention_mask\n","        attention_mask = item['attention_mask']\n","        attention_mask = attention_mask[:max_length] + [0] * max(0, max_length - len(attention_mask))  # Pad with 0\n","        padded_batch[\"attention_mask\"].append(attention_mask)\n","\n","        # Pad or truncate labels\n","        labels = item['labels']\n","        labels = labels[:max_length] + [-100] * max(0, max_length - len(labels))  # Pad with -100 for labels\n","        padded_batch[\"labels\"].append(labels)\n","\n","    # Convert lists to PyTorch tensors\n","    padded_batch[\"input_ids\"] = torch.tensor(padded_batch[\"input_ids\"], dtype=torch.long)\n","    padded_batch[\"attention_mask\"] = torch.tensor(padded_batch[\"attention_mask\"], dtype=torch.long)\n","    padded_batch[\"labels\"] = torch.tensor(padded_batch[\"labels\"], dtype=torch.long)\n","\n","    return padded_batch\n","\n","# Custom data collator\n","def custom_data_collator(batch):\n","    \"\"\"Custom data collator for batching and padding.\"\"\"\n","    return custom_pad_and_truncate(batch, max_length=48)\n","\n","# Metrics calculation function remains the same\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Extract true predictions and labels, ignoring the padding (-100)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    # Flatten the lists of true predictions and labels\n","    true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n","    true_labels_flat = [item for sublist in true_labels for item in sublist]\n","\n","    # Set zero_division to 0 to handle undefined precision/recall\n","    report = classification_report(true_labels_flat, true_predictions_flat, output_dict=True, zero_division=0)\n","    return {\n","        \"precision\": report[\"macro avg\"][\"precision\"],\n","        \"recall\": report[\"macro avg\"][\"recall\"],\n","        \"f1\": report[\"macro avg\"][\"f1-score\"],\n","        \"accuracy\": report[\"accuracy\"],\n","    }\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Ensure all inputs are on the correct device\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        # Debug print for the first training step (ensure minimal operations)\n","        if self.state.global_step == 0:  # Print only during the first step\n","            print(\"\\n[DEBUG] Trainer Input:\")\n","            for key, value in inputs.items():\n","                print(f\"{key}: {value.shape}, dtype: {value.dtype}\")\n","\n","                # Use `detach` and `cpu` to safely access the data for printing without affecting training\n","                detached_value = value.detach().cpu()\n","                for i in range(min(3, detached_value.size(0))):  # Limit to the first 3 inputs\n","                    print(f\"Sample {i + 1} of {key}: {detached_value[i][:10].tolist()}\")  # Convert to list for clearer print\n","\n","        # Forward pass through the model\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = inputs['labels']\n","\n","        # Compute loss\n","        loss = self.label_smoother(outputs, labels) if self.label_smoother else outputs.loss\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["d754eaf1abf841789df1fe3f42bdba15","e94c6fffddb64e9c9e233919ae875da2","6e9b1c5a27e04c838ffb00e34cada9dc","2778a78f1695419c9f4e62bdd96fc390","7506a334ea73474287812f10baaf0854","622504d533b44450948f671555c77699","497920751b924c868925cd029556453b","1bb0c68c676c4719af0909cbeeee13e4","16da41f126a34786b26d0f4887bb7891","2087b66ea84642008a4851d6b19e501c","293f01f1f585419c914f1665fe138482","87783769b0f8492ba5bca4e27526b784","14a1199eb4bc44fe83e5a97ee9e0468b","0766298196044f93af424871b0bf4360","0548975ff0c14919b5487f2f87cc47b4","8c0927e60cc54de69e681dc86479a306","29211b8fbe0f4f888c6121b35ecf93b8","1138c8a67ca743d684287d98b6e60ad8","61a4489dc0424fbc9ae773497135c24f","6089e515acfa449b8da9967bea2601a2","0db5d984f8314d6f8246da403996cb39","8c111f86fb62442eb95c9aef8484a8f0"]},"executionInfo":{"elapsed":7443,"status":"ok","timestamp":1730114722988,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"6exeabfXc7iq","outputId":"e492f144-bfc6-4150-d5d6-52c78ed99714"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d754eaf1abf841789df1fe3f42bdba15","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87783769b0f8492ba5bca4e27526b784","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at Lianglab/PharmBERT-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load model and move to the correct device\n","model = AutoModelForTokenClassification.from_pretrained(\"Lianglab/PharmBERT-uncased\", num_labels=len(label_list))\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Split the data\n","train_data, eval_data = train_test_split(updated_data, test_size=0.2,random_state=42)\n","\n","# Create DataLoaders for training and evaluation using the custom data collator\n","train_dataloader = DataLoader(train_data, batch_size=16, collate_fn=custom_data_collator)\n","eval_dataloader = DataLoader(eval_data, batch_size=16, collate_fn=custom_data_collator)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=3e-5,  # Increased learning rate\n","    per_device_train_batch_size=8,  # Reduced batch size\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    report_to='none'\n",")\n","\n","# Training and model setup remains the same\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data,  # Use the Dataset objects here\n","    eval_dataset=eval_data,\n","    compute_metrics=compute_metrics,\n","    data_collator=custom_data_collator,\n","    tokenizer=tokenizer,  # Still using `tokenizer` for consistency\n",")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1730114725690,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"XWzpeXymn4hl","outputId":"f2058115-0176-44ce-bc15-651c4c866113"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n","input_ids shape: torch.Size([16, 48]), dtype: torch.int64\n","attention_mask shape: torch.Size([16, 48]), dtype: torch.int64\n","labels shape: torch.Size([16, 48]), dtype: torch.int64\n","\n","--- Sample Entry from First Batch ---\n","Input IDs: [70, 801, 144, 1653, 1130, 1068, 897, 1, 1130, 912, 272, 1511, 1565, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Tokens: ['110', 'Metoprolol Succinate', '50,000', 'mg-300', 'Tablet,', 'Sildenafil', 'Onychomycosis', '(10)', 'Tablet,', 'PRN', 'BPH', 'formation', 'hyperthyroidism.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11, 9, 0, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["# Check the shapes and keys of the first batch from the train_dataloader\n","for batch in train_dataloader:\n","    # Move batch tensors to the appropriate device\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","\n","    # Print the keys in the batch dictionary\n","    print(f\"Batch keys: {batch.keys()}\")  # Should show the keys present in the batch (e.g., input_ids, attention_mask, labels)\n","\n","    # Print the shape of each tensor in the batch\n","    for key in batch:\n","        print(f\"{key} shape: {batch[key].shape}, dtype: {batch[key].dtype}\")\n","\n","    # Display a single entry from the batch for validation\n","    print(\"\\n--- Sample Entry from First Batch ---\")\n","    print(\"Input IDs:\", batch['input_ids'][0].tolist())\n","    print(\"Tokens:\", [new_tokens[id] if id < len(new_tokens) and id != 0 else \"[PAD]\" if id == 0 else \"[UNK]\" for id in batch['input_ids'][0].tolist()])\n","    print(\"Labels:\", batch['labels'][0].tolist())\n","    print(\"Attention Mask:\", batch['attention_mask'][0].tolist())\n","\n","    # Break after inspecting the first batch\n","    break"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"collapsed":true,"executionInfo":{"elapsed":404216,"status":"ok","timestamp":1730115135063,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"BnjOC5C3636m","outputId":"71211995-1600-46b2-fac7-507e56e24926"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[DEBUG] Trainer Input:\n","input_ids: torch.Size([8, 48]), dtype: torch.int64\n","Sample 1 of input_ids: [158, 324, 84, 1653, 1130, 1068, 897, 1, 1130, 912]\n","Sample 2 of input_ids: [117, 1043, 64, 1653, 1130, 1068, 897, 1, 1130, 912]\n","Sample 3 of input_ids: [131, 801, 144, 1653, 1130, 1068, 897, 1, 1130, 912]\n","attention_mask: torch.Size([8, 48]), dtype: torch.int64\n","Sample 1 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Sample 2 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Sample 3 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","labels: torch.Size([8, 48]), dtype: torch.int64\n","Sample 1 of labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11]\n","Sample 2 of labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11]\n","Sample 3 of labels: [0, 1, 3, 4, 5, 0, 7, 7, 5, 11]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7515' max='7515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7515/7515 06:34, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.032800</td>\n","      <td>0.026759</td>\n","      <td>0.988846</td>\n","      <td>0.992355</td>\n","      <td>0.990573</td>\n","      <td>0.992441</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.018000</td>\n","      <td>0.021691</td>\n","      <td>0.992344</td>\n","      <td>0.994512</td>\n","      <td>0.993404</td>\n","      <td>0.995049</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.013300</td>\n","      <td>0.015940</td>\n","      <td>0.994206</td>\n","      <td>0.995949</td>\n","      <td>0.995062</td>\n","      <td>0.995943</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.008900</td>\n","      <td>0.015288</td>\n","      <td>0.994825</td>\n","      <td>0.996812</td>\n","      <td>0.995810</td>\n","      <td>0.996667</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.007100</td>\n","      <td>0.014676</td>\n","      <td>0.995144</td>\n","      <td>0.996701</td>\n","      <td>0.995916</td>\n","      <td>0.996812</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizer vocabulary saved to ./results/checkpoint-1503/vocab.json\n","Tokenizer vocabulary saved to ./results/checkpoint-3006/vocab.json\n","Tokenizer vocabulary saved to ./results/checkpoint-4509/vocab.json\n","Tokenizer vocabulary saved to ./results/checkpoint-6012/vocab.json\n","Tokenizer vocabulary saved to ./results/checkpoint-7515/vocab.json\n","Tokenizer vocabulary saved to ./results/checkpoint-7515/vocab.json\n","Tokenizer vocabulary saved to ./split_finetuned_pharmbert_model/vocab.json\n"]}],"source":["# Train and save the model\n","trainer.train()\n","trainer.save_model(\"./split_finetuned_pharmbert_model\")"]},{"cell_type":"markdown","metadata":{"id":"PKTskA73QMc4"},"source":["# **EXAMPLE TEST**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28957,"status":"ok","timestamp":1730106001292,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"5MJzTFspFNUQ","outputId":"77b77d7b-3dc2-4594-905e-a199e27a9ff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install datasets\n","\n","%cd /content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1730115515045,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"ea6oDVa862ML"},"outputs":[],"source":["# Initialize a set to collect unique words from the data\n","new_tokens_set = set()\n","\n","# Simple space-based tokenizer function for vocabulary extraction\n","def simple_tokenizer(text: str):\n","    return text.split()\n","\n","# Iterate through each entry in the aligned_data\n","for entry in aligned_data:\n","    # Extract the instructions and split into words\n","    instruction_tokens = simple_tokenizer(entry['instructions'])\n","    new_tokens_set.update(instruction_tokens)  # Add tokens to the set\n","\n","    # Extract medication details\n","    medications = entry['medications']\n","\n","    # Iterate through each medication in the entry\n","    for medication_name, fields in medications.items():\n","        # Add the medication name itself\n","        new_tokens_set.add(medication_name)\n","\n","        # Iterate through each field in the medication dictionary\n","        for field_name, field_value in fields.items():\n","            # Split the field value into tokens and add them\n","            field_tokens = simple_tokenizer(field_value)\n","            new_tokens_set.update(field_tokens)\n","\n","# Convert the set to a sorted list to maintain a consistent order\n","new_tokens = sorted(list(new_tokens_set))\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11638,"status":"ok","timestamp":1730115724481,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"wDKxX-1UO5vn","outputId":"2f5b9a61-c9bc-40c9-b0f3-eba3ce62a8c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 20 records\n","\n","--- Sample Entry from Record 1 ---\n","Tokens: ['1.', 'Fluticasone-Salmeterol', '250-50', 'mcg/Dose', 'Disk', 'with', 'Device', 'Sig:', 'One', '(1)', 'Disk', 'with', 'Device', 'Inhalation', '[**Hospital1', '**]', '(2', 'times', 'a', 'day).', 'Disp:*60', 'Disk', 'with', 'Device(s)*', 'Refills:*2*']\n","Ground Truth Labels: ['O', 'B-DRUG', 'B-STRENGTH', 'I-STRENGTH', 'B-FORM', 'O', 'B-FORM', 'O', 'B-DOSAGE', 'I-DOSAGE', 'B-FORM', 'O', 'I-FORM', 'B-ROUTE', 'O', 'O', 'B-DOSAGE', 'I-DOSAGE', 'I-DOSAGE', 'B-FREQUENCY', 'O', 'B-FORM', 'O', 'B-FORM', 'O']\n","Predicted Labels  : ['O', 'B-DRUG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'B-DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'B-DOSAGE', 'B-DOSAGE', 'O']\n","Processed 20 records\n","Processed 20 records\n","Processed 20 records\n","Processed 20 records\n","\n","Overall Accuracy per Field:\n","O              : 0.98\n","B-DRUG         : 0.68\n","I-DRUG         : 0.00\n","B-STRENGTH     : 0.83\n","I-STRENGTH     : 0.87\n","B-FORM         : 0.60\n","I-FORM         : 0.00\n","B-DOSAGE       : 0.76\n","I-DOSAGE       : 0.00\n","B-FREQUENCY    : 0.44\n","I-FREQUENCY    : 0.00\n","B-ROUTE        : 0.78\n","I-ROUTE        : 0.00\n","B-REASON       : 0.48\n","I-REASON       : 0.10\n"]}],"source":["import os\n","import json\n","import torch\n","import numpy as np\n","\n","# Define label list\n","label_list = [\"O\", \"B-DRUG\", \"I-DRUG\", \"B-STRENGTH\", \"I-STRENGTH\",\n","              \"B-FORM\", \"I-FORM\", \"B-DOSAGE\", \"I-DOSAGE\",\n","              \"B-FREQUENCY\", \"I-FREQUENCY\", \"B-ROUTE\", \"I-ROUTE\",\n","              \"B-REASON\", \"I-REASON\"]\n","\n","# Check if a GPU is available and set the device accordingly\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load your custom tokenizer class\n","class SimpleTokenizer:\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","        self.token_to_id = {token: idx + 1 for idx, token in enumerate(vocab)}  # ID starts at 1\n","        self.unknown_token_id = len(self.token_to_id) + 1  # ID for unknown tokens\n","\n","    def tokenize(self, text: str) -> list:\n","        return text.split()  # Simple space-based tokenization\n","\n","    def convert_tokens_to_ids(self, tokens: list) -> list:\n","        return [self.token_to_id.get(token, self.unknown_token_id) for token in tokens]\n","\n","    def __call__(self, text: str, **kwargs) -> dict:\n","        tokens = self.tokenize(text)\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": [1] * len(input_ids)  # Default attention mask for each token\n","        }\n","\n","# Initialize your SimpleTokenizer with your vocabulary, new_tokens defined above\n","tokenizer = SimpleTokenizer(vocab=new_tokens)\n","\n","# Function to predict and compare labels\n","def predict_and_evaluate(tokens, ground_truth_labels, model, tokenizer, label_list, max_length=48):\n","    # Tokenize input using your SimpleTokenizer\n","    inputs = tokenizer(' '.join(tokens))  # Join tokens into a single string for tokenization\n","\n","    # Pad or truncate input IDs and attention masks\n","    input_ids = inputs['input_ids'][:max_length] + [0] * max(0, max_length - len(inputs['input_ids']))\n","    attention_mask = inputs['attention_mask'][:max_length] + [0] * max(0, max_length - len(inputs['attention_mask']))\n","\n","    # Convert to tensors and move to device\n","    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension and move to device\n","    attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n","\n","    # Get model predictions\n","    model.eval()  # Put the model in evaluation mode\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","    # Get the predicted labels\n","    predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()\n","\n","    # Convert predicted label ids to actual label names\n","    predicted_labels = [label_list[p] for p in predictions[0]]\n","\n","    return tokens, predicted_labels  # Return tokens and predicted labels\n","\n","# Initialize accuracy counters\n","correct_preds = {label: 0 for label in label_list}\n","total_preds = {label: 0 for label in label_list}\n","\n","# Read the JSON files and evaluate the predictions\n","json_folder = \"test/labeled_records\"\n","all_records = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n","\n","# Flag to control sample entry printing\n","printed_sample = False\n","record_count = 0\n","\n","for record_file in all_records:\n","    # Print statement for every 20 records processed\n","    if record_count % 20 == 0:\n","        print(f\"Processed 20 records\")  # Print statement to show record processing\n","\n","    with open(os.path.join(json_folder, record_file), 'r') as json_file:\n","        data = json.load(json_file)\n","        entries = data['entries']\n","\n","        for entry in entries:\n","            tokens = entry['tokens']\n","            ground_truth_labels = entry['labels']\n","\n","            # Get model predictions\n","            predicted_tokens, predicted_labels = predict_and_evaluate(\n","                tokens, ground_truth_labels, model, tokenizer, label_list\n","            )\n","\n","            # Update accuracy counts\n","            for pred_label, true_label in zip(predicted_labels, ground_truth_labels):\n","                if true_label in label_list:  # Ensure it's a valid label to evaluate\n","                    total_preds[true_label] += 1\n","                    if pred_label == true_label:\n","                        correct_preds[true_label] += 1\n","\n","            # Show sample entry only for the first record\n","            if not printed_sample:\n","                print(f\"\\n--- Sample Entry from Record 1 ---\")\n","                print(f\"Tokens: {tokens}\")\n","                print(f\"Ground Truth Labels: {ground_truth_labels}\")\n","                print(f\"Predicted Labels  : {predicted_labels}\")\n","                printed_sample = True  # Set flag to True after printing the first sample\n","\n","    record_count += 1  # Increment the record count after processing each file\n","\n","# Calculate overall accuracy for each label\n","overall_accuracies = {label: (correct_preds[label] / total_preds[label]) if total_preds[label] > 0 else 0 for label in label_list}\n","\n","# Display the overall accuracy for each field\n","print(\"\\nOverall Accuracy per Field:\")\n","for label, accuracy in overall_accuracies.items():\n","    print(f\"{label:15}: {accuracy:.2f}\")\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":650,"status":"ok","timestamp":1730115734248,"user":{"displayName":"Remon Muhana","userId":"00925872298215185676"},"user_tz":-660},"id":"18Mziau5UvcT","outputId":"ec22d18c-17aa-48a5-834f-49c78c36005d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Prediction for Single Example ---\n","2.: O\n","Rosuvastatin: B-DRUG\n","10: B-STRENGTH\n","mg: I-STRENGTH\n","Tablet: B-FORM\n","Sig:: O\n","One: B-DOSAGE\n","(1): B-DOSAGE\n","Tablet: B-FORM\n","PO: B-ROUTE\n","QHS: B-FREQUENCY\n","for: O\n","high: B-REASON\n","cholesterol.: B-REASON\n"]}],"source":["import torch\n","import numpy as np\n","from transformers import AutoModelForTokenClassification\n","\n","# Define label list\n","label_list = [\"O\", \"B-DRUG\", \"I-DRUG\", \"B-STRENGTH\", \"I-STRENGTH\",\n","              \"B-FORM\", \"I-FORM\", \"B-DOSAGE\", \"I-DOSAGE\",\n","              \"B-FREQUENCY\", \"I-FREQUENCY\", \"B-ROUTE\", \"I-ROUTE\",\n","              \"B-REASON\", \"I-REASON\"]\n","\n","# Load model\n","model = AutoModelForTokenClassification.from_pretrained(\"./split_finetuned_pharmbert_model\", num_labels=len(label_list))\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define your custom SimpleTokenizer\n","class SimpleTokenizer:\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","        self.token_to_id = {token: idx + 1 for idx, token in enumerate(vocab)}  # ID starts at 1\n","        self.unknown_token_id = len(self.token_to_id) + 1  # ID for unknown tokens\n","\n","    def tokenize(self, text: str) -> list:\n","        return text.split()  # Simple space-based tokenization\n","\n","    def convert_tokens_to_ids(self, tokens: list) -> list:\n","        return [self.token_to_id.get(token, self.unknown_token_id) for token in tokens]\n","\n","    def __call__(self, text: str):\n","        tokens = self.tokenize(text)\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": [1] * len(input_ids)  # Default attention mask for each token\n","        }\n","\n","# Initialize your tokenizer with the new vocabulary\n","# Assuming 'new_tokens' is the list of unique tokens you've created previously\n","tokenizer = SimpleTokenizer(vocab=new_tokens)\n","\n","# Function to predict labels using the SimpleTokenizer\n","def predict_single_example_untokenized(input_example, model, tokenizer, label_list, max_length=48):\n","    # Use simple space-based tokenizer\n","    simple_tokens = tokenizer.tokenize(input_example)\n","\n","    # Prepare input tensor\n","    inputs = tokenizer(input_example)\n","\n","    # Convert input_ids to tensor\n","    input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(device)  # Add batch dimension and move to device\n","    attention_mask = torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0).to(device)  # Add batch dimension\n","\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","    # Get the predicted labels\n","    predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()\n","\n","    # Convert predicted label ids to actual label names\n","    predicted_labels = [label_list[p] for p in predictions[0]]\n","\n","    # Align the length of predicted labels with simple tokens\n","    predicted_labels = predicted_labels[:len(simple_tokens)]\n","\n","    # Print the results\n","    print(\"\\n--- Prediction for Single Example ---\")\n","    for token, label in zip(simple_tokens, predicted_labels):\n","        print(f\"{token}: {label}\")\n","\n","# Manually provide an untokenized example input\n","example_input = \"2. Rosuvastatin 10 mg Tablet Sig: One (1) Tablet PO QHS for high cholesterol.\"\n","\n","# Run the prediction\n","predict_single_example_untokenized(example_input, model, tokenizer, label_list)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"PharmBERT","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0548975ff0c14919b5487f2f87cc47b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0db5d984f8314d6f8246da403996cb39","placeholder":"​","style":"IPY_MODEL_8c111f86fb62442eb95c9aef8484a8f0","value":" 438M/438M [00:04&lt;00:00, 89.6MB/s]"}},"0766298196044f93af424871b0bf4360":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61a4489dc0424fbc9ae773497135c24f","max":438147282,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6089e515acfa449b8da9967bea2601a2","value":438147282}},"0db5d984f8314d6f8246da403996cb39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1138c8a67ca743d684287d98b6e60ad8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14a1199eb4bc44fe83e5a97ee9e0468b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29211b8fbe0f4f888c6121b35ecf93b8","placeholder":"​","style":"IPY_MODEL_1138c8a67ca743d684287d98b6e60ad8","value":"pytorch_model.bin: 100%"}},"16da41f126a34786b26d0f4887bb7891":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bb0c68c676c4719af0909cbeeee13e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2087b66ea84642008a4851d6b19e501c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2778a78f1695419c9f4e62bdd96fc390":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2087b66ea84642008a4851d6b19e501c","placeholder":"​","style":"IPY_MODEL_293f01f1f585419c914f1665fe138482","value":" 724/724 [00:00&lt;00:00, 53.7kB/s]"}},"29211b8fbe0f4f888c6121b35ecf93b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"293f01f1f585419c914f1665fe138482":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"497920751b924c868925cd029556453b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6089e515acfa449b8da9967bea2601a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61a4489dc0424fbc9ae773497135c24f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"622504d533b44450948f671555c77699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e9b1c5a27e04c838ffb00e34cada9dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bb0c68c676c4719af0909cbeeee13e4","max":724,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16da41f126a34786b26d0f4887bb7891","value":724}},"7506a334ea73474287812f10baaf0854":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87783769b0f8492ba5bca4e27526b784":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14a1199eb4bc44fe83e5a97ee9e0468b","IPY_MODEL_0766298196044f93af424871b0bf4360","IPY_MODEL_0548975ff0c14919b5487f2f87cc47b4"],"layout":"IPY_MODEL_8c0927e60cc54de69e681dc86479a306"}},"8c0927e60cc54de69e681dc86479a306":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c111f86fb62442eb95c9aef8484a8f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d754eaf1abf841789df1fe3f42bdba15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e94c6fffddb64e9c9e233919ae875da2","IPY_MODEL_6e9b1c5a27e04c838ffb00e34cada9dc","IPY_MODEL_2778a78f1695419c9f4e62bdd96fc390"],"layout":"IPY_MODEL_7506a334ea73474287812f10baaf0854"}},"e94c6fffddb64e9c9e233919ae875da2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_622504d533b44450948f671555c77699","placeholder":"​","style":"IPY_MODEL_497920751b924c868925cd029556453b","value":"config.json: 100%"}}}}},"nbformat":4,"nbformat_minor":0}
