{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files concatenated into /Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/concatenated.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def concatenate_txt_files(input_folder, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Iterate over each file in the input folder\n",
    "        for filename in sorted(os.listdir(input_folder)):\n",
    "            # Check if the file is a .txt file\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "                # Use ISO-8859-1 encoding to read files with unknown or mixed encodings\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1', errors='ignore') as infile:\n",
    "                    # Read each line in the file and write it to the output file\n",
    "                    for line in infile:\n",
    "                        outfile.write(line)\n",
    "                    # Add a newline after each file's content (optional)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "input_folder = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/test_dataset/input'  # Replace with your folder path\n",
    "output_file = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/concatenated.txt'  # Name of the output file\n",
    "concatenate_txt_files(input_folder, output_file)\n",
    "\n",
    "print(f\"All files concatenated into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/test_dataset/input'  # Replace with your folder path\n",
    "output_file = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/concatenated.txt'  # Name of the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data has been saved to '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/concatenated.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Lianglab/PharmBERT-uncased\")\n",
    "\n",
    "# Function to load raw instructions from the input directory\n",
    "def load_raw_instructions(input_dir):\n",
    "    \"\"\"Load raw instructions from input files line by line.\"\"\"\n",
    "    input_data = {}\n",
    "    entry_index = 0  # Use an index to track unique entries\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    cleaned_line = line.strip()\n",
    "                    if cleaned_line:  # Ignore empty lines\n",
    "                        entry_name = f\"{file_name}_{entry_index}\"\n",
    "                        input_data[entry_name] = cleaned_line\n",
    "                        entry_index += 1\n",
    "    return input_data\n",
    "\n",
    "# Function to tokenize the instructions\n",
    "def tokenize_instructions(raw_instructions):\n",
    "    \"\"\"Tokenize raw instructions without alignment.\"\"\"\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for base_name, instruction in raw_instructions.items():\n",
    "        # Tokenize the raw instruction\n",
    "        tokenized_inputs = tokenizer(\n",
    "            instruction.split(),\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=48,\n",
    "            padding='max_length',\n",
    "        )\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0])\n",
    "\n",
    "        # Prepare entry for saving\n",
    "        entry = {\n",
    "            \"raw_instruction\": instruction,\n",
    "            \"tokenized_instruction\": tokens\n",
    "        }\n",
    "\n",
    "        tokenized_data.append(entry)\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Function to save the data to JSON\n",
    "def save_to_json(data, output_file):\n",
    "    \"\"\"Save the tokenized data to a JSON file.\"\"\"\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Tokenized data has been saved to '{output_file}'.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input directory and output JSON file\n",
    "    input_dir = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/test_dataset/input'  # Update with your input directory path\n",
    "    output_json = '/Users/remon.m/Desktop/Y4S2/Thesis/Coding/Synth/concatenated.json'\n",
    "\n",
    "    # Load raw instructions\n",
    "    raw_instructions = load_raw_instructions(input_dir)\n",
    "\n",
    "    # Tokenize instructions\n",
    "    tokenized_data = tokenize_instructions(raw_instructions)\n",
    "\n",
    "    # Save to JSON\n",
    "    save_to_json(tokenized_data, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PharmBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
