{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1730119794874,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "T7a6xaG7HyI7",
    "outputId": "c8562a42-ceb2-4595-cc24-4c6a00721015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "/content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "%cd /content/drive/Othercomputers/My MacBook Air/Thesis/Coding/Synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730119794874,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "jWnUZDQyFFC6"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "\n",
    "# Hugging Face Datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1730119795848,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "R9rXoiLnFikf"
   },
   "outputs": [],
   "source": [
    "with open('final_data.pkl', 'rb') as file:\n",
    "    final_data = pickle.load(file)\n",
    "\n",
    "with open('raw_data.pkl', 'rb') as file:\n",
    "    raw_data = pickle.load(file)\n",
    "\n",
    "with open('aligned_tokenized_data.pkl', 'rb') as file:\n",
    "    aligned_tokenized_data = pickle.load(file)\n",
    "\n",
    "# Split the data into training and evaluation sets\n",
    "train_data, eval_data = train_test_split(final_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a set to collect unique words from the data\n",
    "new_tokens_set = set()\n",
    "\n",
    "# Simple space-based tokenizer function for vocabulary extraction\n",
    "def simple_tokenizer(text: str):\n",
    "    return text.split()\n",
    "\n",
    "# Iterate through each entry in the raw_data\n",
    "for entry in raw_data:\n",
    "    # Extract the instructions and split into words\n",
    "    instruction_tokens = simple_tokenizer(entry['instructions'])\n",
    "    new_tokens_set.update(instruction_tokens)  # Add tokens to the set\n",
    "\n",
    "    # Extract medication details\n",
    "    medications = entry['medications']\n",
    "\n",
    "    # Iterate through each medication in the entry\n",
    "    for medication_name, fields in medications.items():\n",
    "        # Add the medication name itself\n",
    "        new_tokens_set.add(medication_name)\n",
    "\n",
    "        # Iterate through each field in the medication dictionary\n",
    "        for field_name, field_value in fields.items():\n",
    "            # Split the field value into tokens and add them\n",
    "            field_tokens = simple_tokenizer(field_value)\n",
    "            new_tokens_set.update(field_tokens)\n",
    "\n",
    "# Convert the set to a sorted list to maintain a consistent order\n",
    "new_tokens = sorted(list(new_tokens_set))\n",
    "\n",
    "# Define label list\n",
    "label_list = [\"O\", \"B-DRUG\", \"I-DRUG\", \"B-STRENGTH\", \"I-STRENGTH\",\n",
    "              \"B-FORM\", \"I-FORM\", \"B-DOSAGE\", \"I-DOSAGE\",\n",
    "              \"B-FREQUENCY\", \"I-FREQUENCY\", \"B-ROUTE\", \"I-ROUTE\",\n",
    "              \"B-REASON\", \"I-REASON\"]\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./split_finetuned_tiny_clinicalbert_model\", num_labels=len(label_list))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Custom Tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.token_to_id = {token: idx + 1 for idx, token in enumerate(vocab)}  # ID starts at 1\n",
    "        self.unknown_token_id = len(self.token_to_id) + 1  # ID for unknown tokens\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Use the same splitting approach as simple_tokenizer\n",
    "        return text.split()\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        # Handle unknown tokens consistently\n",
    "        return [self.token_to_id.get(token, self.unknown_token_id) for token in tokens]\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        tokens = self.tokenize(text)\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": [1] * len(input_ids)  # Default attention mask for each token\n",
    "        }\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Save the tokenizer vocabulary to a directory.\"\"\"\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        vocab_file = os.path.join(save_directory, 'vocab.json')\n",
    "        with open(vocab_file, 'w') as f:\n",
    "            json.dump(self.token_to_id, f)\n",
    "\n",
    "        print(f\"Tokenizer vocabulary saved to {vocab_file}\")\n",
    "\n",
    "# Initialize the tokenizer with the new vocabulary\n",
    "tokenizer = SimpleTokenizer(vocab=new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730119795848,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "c1qJDkVaHMfh"
   },
   "outputs": [],
   "source": [
    "# Custom padding function\n",
    "def custom_pad_and_truncate(batch, max_length=48):\n",
    "    \"\"\"Pad and truncate the input IDs, attention masks, and labels.\"\"\"\n",
    "    padded_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for item in batch:\n",
    "        # Pad or truncate input_ids\n",
    "        input_ids = item['input_ids']\n",
    "        input_ids = input_ids[:max_length] + [0] * max(0, max_length - len(input_ids))  # Pad with 0\n",
    "        padded_batch[\"input_ids\"].append(input_ids)\n",
    "\n",
    "        # Pad or truncate attention_mask\n",
    "        attention_mask = item['attention_mask']\n",
    "        attention_mask = attention_mask[:max_length] + [0] * max(0, max_length - len(attention_mask))  # Pad with 0\n",
    "        padded_batch[\"attention_mask\"].append(attention_mask)\n",
    "\n",
    "        # Pad or truncate labels\n",
    "        labels = item['labels']\n",
    "        labels = labels[:max_length] + [-100] * max(0, max_length - len(labels))  # Pad with -100 for labels\n",
    "        padded_batch[\"labels\"].append(labels)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    padded_batch[\"input_ids\"] = torch.tensor(padded_batch[\"input_ids\"], dtype=torch.long)\n",
    "    padded_batch[\"attention_mask\"] = torch.tensor(padded_batch[\"attention_mask\"], dtype=torch.long)\n",
    "    padded_batch[\"labels\"] = torch.tensor(padded_batch[\"labels\"], dtype=torch.long)\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "# Custom data collator\n",
    "def custom_data_collator(batch):\n",
    "    \"\"\"Custom data collator for batching and padding.\"\"\"\n",
    "    return custom_pad_and_truncate(batch, max_length=48)\n",
    "\n",
    "# Metrics calculation function remains the same\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Extract true predictions and labels, ignoring the padding (-100)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Flatten the lists of true predictions and labels\n",
    "    true_predictions_flat = [item for sublist in true_predictions for item in sublist]\n",
    "    true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Set zero_division to 0 to handle undefined precision/recall\n",
    "    report = classification_report(true_labels_flat, true_predictions_flat, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Ensure all inputs are on the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Debug print for the first training step (ensure minimal operations)\n",
    "        if self.state.global_step == 0:  # Print only during the first step\n",
    "            print(\"\\n[DEBUG] Trainer Input:\")\n",
    "            for key, value in inputs.items():\n",
    "                print(f\"{key}: {value.shape}, dtype: {value.dtype}\")\n",
    "\n",
    "                # Use `detach` and `cpu` to safely access the data for printing without affecting training\n",
    "                detached_value = value.detach().cpu()\n",
    "                for i in range(min(3, detached_value.size(0))):  # Limit to the first 3 inputs\n",
    "                    print(f\"Sample {i + 1} of {key}: {detached_value[i][:10].tolist()}\")  # Convert to list for clearer print\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs['labels']\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.label_smoother(outputs, labels) if self.label_smoother else outputs.loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1730119796477,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "S82Qc4BoFnY0"
   },
   "outputs": [],
   "source": [
    "# Load your fine-tuned model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./split_finetuned_tiny_clinicalbert_model\", num_labels=len(label_list))\n",
    "\n",
    "### Step 1: Pruning ###\n",
    "# Prune the linear layers of the fine-tuned model\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.3)  # Prune 30% of weights\n",
    "        prune.remove(module, 'weight')  # Permanently remove the pruned weights\n",
    "\n",
    "# Save the pruned model\n",
    "model.save_pretrained(\"./pruned_finetuned_tiny_clinicalbert_model\")\n",
    "\n",
    "# Load the pruned model for further training\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./pruned_finetuned_tiny_clinicalbert_model\", num_labels=len(label_list))\n",
    "\n",
    "# Create DataLoaders for training and evaluation using the custom data collator\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, collate_fn=custom_data_collator)\n",
    "eval_dataloader = DataLoader(eval_data, batch_size=16, collate_fn=custom_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "executionInfo": {
     "elapsed": 173034,
     "status": "ok",
     "timestamp": 1730119969502,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "Jj8Kq_LiKY_G",
    "outputId": "710fe0c0-0e7d-4635-baf9-5d3b7efc0636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Trainer Input:\n",
      "input_ids: torch.Size([8, 48]), dtype: torch.int64\n",
      "Sample 1 of input_ids: [158, 1162, 88, 1635, 671, 1068, 897, 1, 667, 413]\n",
      "Sample 2 of input_ids: [117, 730, 30, 531, 467, 1068, 897, 1, 466, 1575]\n",
      "Sample 3 of input_ids: [131, 1162, 88, 1635, 671, 1068, 1198, 4, 1812, 667]\n",
      "attention_mask: torch.Size([8, 48]), dtype: torch.int64\n",
      "Sample 1 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 2 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 3 of attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels: torch.Size([8, 48]), dtype: torch.int64\n",
      "Sample 1 of labels: [0, 1, 3, 4, 5, 0, 7, 7, 11, 9]\n",
      "Sample 2 of labels: [0, 1, 7, 0, 0, 0, 7, 7, 4, 0]\n",
      "Sample 3 of labels: [0, 1, 3, 4, 5, 0, 7, 7, 0, 11]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7515' max='7515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7515/7515 02:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.994014</td>\n",
       "      <td>0.997255</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>0.996241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.995080</td>\n",
       "      <td>0.997584</td>\n",
       "      <td>0.996319</td>\n",
       "      <td>0.996651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.013046</td>\n",
       "      <td>0.994999</td>\n",
       "      <td>0.997498</td>\n",
       "      <td>0.996237</td>\n",
       "      <td>0.996458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.995852</td>\n",
       "      <td>0.997445</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>0.996723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.011990</td>\n",
       "      <td>0.995882</td>\n",
       "      <td>0.997472</td>\n",
       "      <td>0.996673</td>\n",
       "      <td>0.996868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-1503/vocab.json\n",
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-3006/vocab.json\n",
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-4509/vocab.json\n",
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-6012/vocab.json\n",
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-7515/vocab.json\n",
      "Tokenizer vocabulary saved to ./retrained_pruned_model/checkpoint-7515/vocab.json\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Fine-tuning the Pruned Model ###\n",
    "# Re-fine-tune to recover accuracy after pruning (optional)\n",
    "# Setup your training dataset and arguments here\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./retrained_pruned_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,  \n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_data_collator,\n",
    "    tokenizer=tokenizer,  \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the re-fine-tuned pruned model\n",
    "model.save_pretrained(\"./retrained_pruned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1181,
     "status": "ok",
     "timestamp": 1730120805544,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "E20087fbKad3",
    "outputId": "eb91c1d0-41ae-4397-e6d3-77e1a73673af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-0ce70f8ecc9c>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quantized_model = torch.load(\"./quantized_pruned_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=312, out_features=312, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=312, out_features=312, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=312, out_features=312, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=312, out_features=312, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=312, out_features=1200, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=1200, out_features=312, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=312, out_features=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 3: Quantization ###\n",
    "# Load the re-fine-tuned pruned model\n",
    "pruned_model = AutoModelForTokenClassification.from_pretrained(\"./retrained_pruned_model\", num_labels=len(label_list))\n",
    "\n",
    "# Apply dynamic quantization to the pruned model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    pruned_model, \n",
    "    {torch.nn.Linear},  # Only quantizing linear layers\n",
    "    dtype=torch.qint8  # Uses 8-bit integer quantization\n",
    ")\n",
    "\n",
    "# Save the quantized model's weights using `state_dict`\n",
    "torch.save(quantized_model, \"./quantized_pruned_model.pth\")\n",
    "\n",
    "quantized_model = torch.load(\"./quantized_pruned_model.pth\")\n",
    "\n",
    "# Move the model to the appropriate device and set it to evaluation mode\n",
    "device = torch.device('cpu')\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()  # Set the model to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73680,
     "status": "ok",
     "timestamp": 1730120881895,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "jzx6AIerJ4pD",
    "outputId": "cbfaa60c-d5b2-486e-f252-63785781b24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 records\n",
      "\n",
      "--- Sample Entry from Record 1 ---\n",
      "Tokens: ['1.', 'Fluticasone-Salmeterol', '250-50', 'mcg/Dose', 'Disk', 'with', 'Device', 'Sig:', 'One', '(1)', 'Disk', 'with', 'Device', 'Inhalation', '[**Hospital1', '**]', '(2', 'times', 'a', 'day).', 'Disp:*60', 'Disk', 'with', 'Device(s)*', 'Refills:*2*']\n",
      "Ground Truth Labels: ['O', 'B-DRUG', 'B-STRENGTH', 'I-STRENGTH', 'B-FORM', 'O', 'B-FORM', 'O', 'B-DOSAGE', 'I-DOSAGE', 'B-FORM', 'O', 'I-FORM', 'B-ROUTE', 'O', 'O', 'B-DOSAGE', 'I-DOSAGE', 'I-DOSAGE', 'B-FREQUENCY', 'O', 'B-FORM', 'O', 'B-FORM', 'O']\n",
      "Predicted Labels  : ['O', 'O', 'B-STRENGTH', 'I-STRENGTH', 'I-STRENGTH', 'O', 'O', 'O', 'B-DOSAGE', 'B-DOSAGE', 'O', 'O', 'O', 'B-ROUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DOSAGE', 'B-DOSAGE', 'O', 'O']\n",
      "Processed 20 records\n",
      "Processed 20 records\n",
      "Processed 20 records\n",
      "Processed 20 records\n",
      "\n",
      "Overall Accuracy per Field:\n",
      "O              : 0.96\n",
      "B-DRUG         : 0.70\n",
      "I-DRUG         : 0.00\n",
      "B-STRENGTH     : 0.82\n",
      "I-STRENGTH     : 0.91\n",
      "B-FORM         : 0.59\n",
      "I-FORM         : 0.00\n",
      "B-DOSAGE       : 0.73\n",
      "I-DOSAGE       : 0.00\n",
      "B-FREQUENCY    : 0.46\n",
      "I-FREQUENCY    : 0.00\n",
      "B-ROUTE        : 0.84\n",
      "I-ROUTE        : 0.00\n",
      "B-REASON       : 0.48\n",
      "I-REASON       : 0.12\n"
     ]
    }
   ],
   "source": [
    "# Function to predict and compare labels\n",
    "def predict_and_evaluate_for_all(tokens, ground_truth_labels, model, tokenizer, label_list, max_length=48):\n",
    "    # Tokenize input using your SimpleTokenizer\n",
    "    inputs = tokenizer(' '.join(tokens))  # Join tokens into a single string for tokenization\n",
    "\n",
    "    # Pad or truncate input IDs and attention masks\n",
    "    input_ids = inputs['input_ids'][:max_length] + [0] * max(0, max_length - len(inputs['input_ids']))\n",
    "    attention_mask = inputs['attention_mask'][:max_length] + [0] * max(0, max_length - len(inputs['attention_mask']))\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()\n",
    "\n",
    "    # Convert predicted label ids to actual label names\n",
    "    predicted_labels = [label_list[p] for p in predictions[0]]\n",
    "\n",
    "    return tokens, predicted_labels  # Return tokens and predicted labels\n",
    "\n",
    "# Initialize accuracy counters\n",
    "correct_preds = {label: 0 for label in label_list}\n",
    "total_preds = {label: 0 for label in label_list}\n",
    "\n",
    "# Read the JSON files and evaluate the predictions\n",
    "json_folder = \"test/labeled_records\"\n",
    "all_records = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "\n",
    "# Flag to control sample entry printing\n",
    "printed_sample = False\n",
    "record_count = 0\n",
    "\n",
    "for record_file in all_records:\n",
    "    # Print statement for every 20 records processed\n",
    "    if record_count % 20 == 0:\n",
    "        print(f\"Processed 20 records\")  # Print statement to show record processing\n",
    "\n",
    "    with open(os.path.join(json_folder, record_file), 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        entries = data['entries']\n",
    "\n",
    "        for entry in entries:\n",
    "            tokens = entry['tokens']\n",
    "            ground_truth_labels = entry['labels']\n",
    "\n",
    "            # Get model predictions\n",
    "            predicted_tokens, predicted_labels = predict_and_evaluate_for_all(\n",
    "                tokens, ground_truth_labels, quantized_model, tokenizer, label_list\n",
    "            )\n",
    "\n",
    "            # Update accuracy counts\n",
    "            for pred_label, true_label in zip(predicted_labels, ground_truth_labels):\n",
    "                if true_label in label_list:  # Ensure it's a valid label to evaluate\n",
    "                    total_preds[true_label] += 1\n",
    "                    if pred_label == true_label:\n",
    "                        correct_preds[true_label] += 1\n",
    "\n",
    "            # Show sample entry only for the first record\n",
    "            if not printed_sample:\n",
    "                print(f\"\\n--- Sample Entry from Record 1 ---\")\n",
    "                print(f\"Tokens: {tokens}\")\n",
    "                print(f\"Ground Truth Labels: {ground_truth_labels}\")\n",
    "                print(f\"Predicted Labels  : {predicted_labels}\")\n",
    "                printed_sample = True  # Set flag to True after printing the first sample\n",
    "\n",
    "    record_count += 1  # Increment the record count after processing each file\n",
    "\n",
    "# Calculate overall accuracy for each label\n",
    "overall_accuracies = {label: (correct_preds[label] / total_preds[label]) if total_preds[label] > 0 else 0 for label in label_list}\n",
    "\n",
    "# Display the overall accuracy for each field\n",
    "print(\"\\nOverall Accuracy per Field:\")\n",
    "for label, accuracy in overall_accuracies.items():\n",
    "    print(f\"{label:15}: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1730121010322,
     "user": {
      "displayName": "Remon Muhana",
      "userId": "00925872298215185676"
     },
     "user_tz": -660
    },
    "id": "VY9sfxOTO0EX",
    "outputId": "d3c528e4-8a29-40bf-aa49-263cfb85f14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model file: 39.59 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace './quantized_pruned_model.pth' with the actual path to your quantized model file\n",
    "quantized_model_path = \"./quantized_pruned_model.pth\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(quantized_model_path):\n",
    "    # Get the size of the model file\n",
    "    model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "    # Convert size to megabytes for easier readability\n",
    "    model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "    print(f\"Size of the model file: {model_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"File not found: {quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg_JZGhyP3_4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PharmBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
